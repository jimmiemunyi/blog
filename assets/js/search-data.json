{
  
    
        "post0": {
            "title": "Tammy Abraham vs Callum Hudson Odoi Classifier",
            "content": "Introduction . Today, I am going to try and build a model that can differentiate between two players from my favourite football team: Tammy Abraham and Odoi. . Straight out of the box, I realize that this is going to be a difficult task for a computer since the players play for the same team (so they will have the same jersey color most of the time), they have the same hair dye color and they pretty much look alike. . Nonetheless, let&#39;s see how a computer manages this task. . Setting Up . Let&#39;s make the necessary imports . from fastai.vision.widgets import * from fastbook import * . For the data, we will be scraping the internet for images of the two players. I will use the Bing Search API because I find it easy to use. . We will use the doc function from fast.ai to check what we need for the function that it provides for Bing: . doc(search_images_bing) . search_images_bing[source] . search_images_bing(key, term, min_sz=128) . So as you see, we need an api key (which you can get from Bing) and the search term. . Lets see if the functions works . results = search_images_bing(key, &#39;Tammy Abraham&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . As you can see, our search returned 150 images. Let&#39;s peek at one of the pictures: . dest = &#39;images/tammy.jpg&#39; download_url(ims[1], dest) . im = Image.open(dest) im.to_thumb(128, 128) . Now since we see that the function is working as expected, let us write the code that will help getting images for the two players. . players = &#39;Tammy Abraham&#39;, &#39;Callum Hudson Odoi&#39; path = Path(&#39;players&#39;) . if not path.exists(): path.mkdir() for o in players: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . As you can see, we got 297 images in total. The above code organizes the images in folders, with the folder name being the name of the player. . fns = get_image_files(path) fns . (#297) [Path(&#39;players/Tammy Abraham/00000119.jpg&#39;),Path(&#39;players/Tammy Abraham/00000068.jpeg&#39;),Path(&#39;players/Tammy Abraham/00000026.jpg&#39;),Path(&#39;players/Tammy Abraham/00000141.jpg&#39;),Path(&#39;players/Tammy Abraham/00000023.jpg&#39;),Path(&#39;players/Tammy Abraham/00000061.jpg&#39;),Path(&#39;players/Tammy Abraham/00000051.jpg&#39;),Path(&#39;players/Tammy Abraham/00000055.jpg&#39;),Path(&#39;players/Tammy Abraham/00000008.jpg&#39;),Path(&#39;players/Tammy Abraham/00000082.jpg&#39;)...] . Sometimes, images get corrupted along the way, so we inspect our results to see if this happened. . failed = verify_images(fns) failed . (#18) [Path(&#39;players/Tammy Abraham/00000025.jpg&#39;),Path(&#39;players/Tammy Abraham/00000043.jpg&#39;),Path(&#39;players/Tammy Abraham/00000129.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000051.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000032.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000038.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000050.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000054.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000146.jpg&#39;),Path(&#39;players/Callum Hudson Odoi/00000016.jpg&#39;)...] . Unfortunately, 18 images were corrupted so let us delete them from our dataset. . failed.map(Path.unlink); . The next process after getting your data is usually creating a DataBlock which is a BluePrint for a DataLoader that we will feed into our model. . We use a validation set of 20% of the total data. For the transformers, we use a RandomResizeCrop and some data augmentation. . players_blk = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(128, min_scale=0.3), batch_tfms=aug_transforms() ) . Now we can create the DataLoader and display some images to see what we are working with. . dls = players_blk.dataloaders(path) . dls.valid.show_batch(max_n=10, nrows=2) . That seems okay, so let us proceed to creating the model and training it. . Creating the Model . I went with a Convolutional Neural Network (good for computer vision stuff) and the resnet34 architecture. I&#39;ll output the error_rate. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.189435 | 2.859934 | 0.527273 | 00:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.870446 | 1.862521 | 0.490909 | 00:07 | . 1 | 0.734076 | 1.400823 | 0.345455 | 00:07 | . 2 | 0.640269 | 0.927152 | 0.272727 | 00:07 | . 3 | 0.601557 | 0.663442 | 0.236364 | 00:07 | . Our model does&#39;t perform well out of the box, its wrong 23.6% of the time, so that&#39;s an accuracy of 76.4% . But remember, these two look very much alike and for an inexperienced observer, he/she might confuse them too. . Let&#39;s look at the confusion matrix to see what our model is getting wrong: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Wow, so most errors come from classifying Tammy Abraham as Callum Hudson Odoi (12 times). It only confused Odoi for Tammy one time. . Let us inspect the top losses: . interp.plot_top_losses(5, nrows=5) . Can we Improve the Model? . I haven&#39;t learned much at this point on improving models but I am going to give it a shot. fast.ai provides us with a nice GUI for data cleaning and that is what I&#39;ll use. . cleaner = ImageClassifierCleaner(learn) cleaner . . So many of these images are confusing even to the Human eye. I deleted some pictures that may be confusing. . for idx in cleaner.delete(): cleaner.fns[idx].unlink() . Now to retrain our model after the data cleaning, I used a more powerful architecture, the resnet50 and used 10 epochs this time. . # retraining the model players_blk = players_blk.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms() ) dls = players_blk.dataloaders(path) learn = cnn_learner(dls, resnet50, metrics=error_rate) learn.fine_tune(10) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.135414 | 3.326896 | 0.490909 | 00:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.576513 | 0.481626 | 0.272727 | 00:10 | . 1 | 0.441237 | 0.723249 | 0.254545 | 00:10 | . 2 | 0.345033 | 0.537463 | 0.181818 | 00:10 | . 3 | 0.274907 | 0.590239 | 0.181818 | 00:09 | . 4 | 0.258264 | 0.665819 | 0.163636 | 00:09 | . 5 | 0.224453 | 0.746396 | 0.163636 | 00:10 | . 6 | 0.188072 | 0.759091 | 0.163636 | 00:09 | . 7 | 0.174902 | 0.722550 | 0.127273 | 00:10 | . 8 | 0.158223 | 0.722072 | 0.127273 | 00:09 | . 9 | 0.142112 | 0.730478 | 0.145455 | 00:10 | . We got better accuracy this time, specifically 85.9% accuracy. Not bad, Let&#39;s see how our model performs with new data. . Predictions . I uploaded some images and this is how it performed: . # creating upload and classify widgets btn_upload = widgets.FileUpload() btn_run = widgets.Button(description=&#39;Classify&#39;) lbl_pred = widgets.Label() out_pl = widgets.Output() # creating the on click listener def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128, 128)) pred, pred_idx, probs = learn.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . # creating a VBox VBox([widgets.Label(&#39;Upload Your Player&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . . It got 5/6 predictions correct, and the wrong one is an image of Tammy from way before, before he dyed his hair. However judging from the probabilities of the predictions, I suspect some of these images I am predicting might have been in the training set. . To solve this, I read that I should use a seperate test set. Still Learning! I&#39;ll make sure I use one on my next model. . Tune in for more pet projects! .",
            "url": "https://jimmiemunyi.github.io/blog/begginer/2020/08/25/Tammy-Abraham-vs-Callum-Hudson-Odoi-Classifier.html",
            "relUrl": "/begginer/2020/08/25/Tammy-Abraham-vs-Callum-Hudson-Odoi-Classifier.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A Simple Snake Classifier",
            "content": "Introduction . In line with my study of the fast.ai course, I decided to build pet projects along the way. Today, I will be building a Classifier that can classify snakes from 5 different species, namely: . Nerodia Sipedon - Northern Watersnake | Thamnophis Sirtalis - Common Garter snake | Storeria Dekayi - DeKay&#39;s Brown snake | Patherophis Obsoletus - Black Rat snake | Crotalus Atrox - Western Diamondback rattlesnake | . While this is just a basic task, I intend to expand this pet project further in future to differentiate between venomous snakes and non-venomous snakes, in addition to their species. . . Getting the Data . The data I use today can be found on Kaggle. . Note: You will need an account and api key to download it. . I do a little preprocessing too, ie, change the parent folders of the snakes to their species. (This will be important later.) . from google.colab import files files.upload() . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d sameeharahman/preprocessed-snake-images !unzip *zip &amp;&amp; rm *zip . %cd preprocessed-cleaned-set/train/ %mv class-1 &#39;Nerodia sipedon&#39; %mv class-2 &#39;Thamnophis sirtalis&#39; %mv class-3 &#39;Storeria dekayi&#39; %mv class-4 &#39;Patherophis obsoletus&#39; %mv class-5 &#39;Crotalus atrox&#39; . Required Installations and Imports . !pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastbook import * from fastai.vision.widgets import * . Converting the Data to DataLoaders . Let&#39;s check out one of the images. . dest = &#39;Crotalus atrox/00211adf56c52e867246185a40859aa2.jpg&#39; im = Image.open(dest) im.to_thumb(128, 128) . Straight out of the box, we see that some of the images might be padded. This a problem. It results in a lot of empty space which is just wasted computation for our model and can result in lower effective resolution for the part of the image we actually use. . The Dataset from Kaggle has been preprocessed by the author. An alternative approach would be to get the original dateset and do the preprocessing myself. However, I will proceed with the current images. . Let&#39;s create the DataBlock, which will be the building block of the DataLoaders. The parameters are as follows: . we provide two blocks, ImageBlock for the independent variable(the images) and CategoryBlock for the dependent variable, y (the label of the images). | we provide a way of getting the Validation Set by using the RandomSplitter parameter and 20% of the data. | we provide a way of getting the labels of each class using the parent_label (that&#39;s why we renamed the folders before to match the species) | we provide some tranformers to be applied to the images which are RandomResizeCrop and data augmentation transformer | . path = Path(&#39;.&#39;) # set path variable to current working directory . snakes = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms() ) . Now we can create the DataLoader. . # creating the dataloaders dls = snakes.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . . CNN Learner . Next, we are going to use a Convolutional Neural Network and use tranfer learning on the resnet34 architecture to create our model. We use 4 epochs. . For the metric, we output the error rate. . # creating the CNN learner learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 0.932205 | 0.610560 | 0.222606 | 01:47 | . epoch train_loss valid_loss error_rate time . 0 | 0.542848 | 0.446825 | 0.163647 | 02:03 | . 1 | 0.385937 | 0.326108 | 0.115617 | 02:04 | . 2 | 0.281081 | 0.292349 | 0.109865 | 02:04 | . 3 | 0.182197 | 0.262394 | 0.092321 | 02:04 | . Out of the box, with fastai library, we get an error rate of 9.23% which equates to 90.77% accuracy!! That is impressive. . Let&#39;s check out the confusion matrix to see how our classifier performed. . interp=ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . We can see that most errors come from our model predicting the Nerodia Sipedon as Patherophis Obsoletus. But the others seem okay. We could improve this by Data Cleaning and preprocessing of the dataset. . Let us check the top losses of our model. . interp.plot_top_losses(5, nrows=5) # i have used 5 rows for visibility purposes . . Predictions . Now let us see how our model does with new data. I will create a widget for uploading an image and classifying the uploaded image. . # creating upload and classify widgets btn_upload = widgets.FileUpload() btn_run = widgets.Button(description=&#39;Classify&#39;) lbl_pred = widgets.Label() out_pl = widgets.Output() # creating the on click listener def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128, 128)) pred, pred_idx, probs = learn.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . I tested each class with an upload and the model got all of them right. I will continue trying to test the model with other images and trying to improve the model. . # creating a VBox VBox([widgets.Label(&#39;Select your snake!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . . Where From Here . This model only recognizes 5 species of snakes. However, snakes are extremely diverse. Currently, snakes are classified into 24 families, containing 528 genera and 3,709 species. . To add to that, some snakes look alike but are from diffrent species and some are venomous while their look alikes are harmless! . As I said in the beggining, my goal would be to build a model that classifies a new image provided as venomous or harmless. . I am terrified of snakes and recognizing which are venomous would go along way!! . I will continue working towards my goal. . Follow my blog to see more of my journey. I plan on documenting most (if not all) of it. .",
            "url": "https://jimmiemunyi.github.io/blog/begginer/2020/08/24/A-Simple-Snake-Classifier.html",
            "relUrl": "/begginer/2020/08/24/A-Simple-Snake-Classifier.html",
            "date": " • Aug 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My Name is Jimmie Munyi. I am hobbyist developer with interested in Machine Learning and Deep Learning. . This is my technical blog where I post everything I encounter in my journey from beginner posts to passion projects. . I am a student, graduation set for 2021. .",
          "url": "https://jimmiemunyi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jimmiemunyi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}