{
  
    
        "post0": {
            "title": "Making A Book Recommendation System",
            "content": "Imports . from fastai.collab import * from fastai.tabular.all import * . Introduction . . Tip: Did you know you can run these tutorials interactively? Simply press the Colab badge on the top. . We are going to use Collaborative Filtering to create a book recommendation system. Recommender Systems based on collaborative make recommendations based on what similar users liked. So if I have read Inferno by Dan Brown and liked it (which I did), the system will look for books also liked by people who liked Inferno and recommend them to me. . It uses a concept called latent factors. What are latent factors you may ask? Let us understand them using an example. . Let us take two books. Inferno by Dan Brown and To Kill A MockingBird by Harper Lee. Those are two very different books. Let us consider three aspects to classify this books: classics (whether the book is a classic or not), action-oriented and new-gen (modern-times setting). . We will give a score ranging from 1 to -1 to all these three aspects of our books, 1 meaning it scores high on that aspect and -1 for a low score. . So, for example, To Kill a MockingBird is a classic, it is not so much action-oriented and also not so much new-gen. So its scores would look as follows: . mocking_bird = np.array([0.9, -0.85, -0.9]) . For Inferno, which is not so much a classic, is action-oriented and is new-gen, we would have the following representation: . inferno = np.array([-0.9, 0.7, 0.9]) . Now, let us hypothesize a reader who doesn&#39;t enjoy classics that much, doesn&#39;t mind action books and really prefers new-gen books (this user actually represents my liking). . reader = np.array([-0.5, 0.2, 0.9]) . To see which book we need to recommend our not-so-much hypothetical reader, we will need to perform dot product between the book factors and the user factors. Dot product simply means multiply each consecutive items and add the results. . (reader*mocking_bird).sum() . -1.4300000000000002 . (reader*inferno).sum() . 1.4 . Based on the results, we would want to recommend Inferno to our user and not To Kill A MockingBird since he wouldn&#39;t enjoy the latter that much. . The magic part about Collaborative Filtering and the reason we called our factors latent is that; we never tell our model anything about what factors to consider or how high or low a book or user scores on each factors. The model learns all that by itself. We simply feed it books rated by users and using that information, it figures out a way to score each factor. As I said, pretty magical. . Let us see that in action. . The DataSet . Luckily, there is already a good dataset on books and ratings by users called GoodBooks10k available from this repo in github. As the name suggests, it contains ratings of 10,000 books and 6 million ratings. . We can download and extract the data using the following convinient functions: . path = Path(&#39;data&#39;) if not path.exists(): path.mkdir() untar_data(&#39;https://github.com/zygmuntz/goodbooks-10k/releases/download/v1.0/goodbooks-10k.zip&#39;, dest=path) . We can inspect what has been downloaded: . path.ls(file_type=&#39;text&#39;) . (#5) [Path(&#39;README.md&#39;),Path(&#39;tags.csv&#39;),Path(&#39;books.csv&#39;),Path(&#39;book_tags.csv&#39;),Path(&#39;ratings.csv&#39;)] . We are interested in the rating.csv file, since that is where the actual ratings are. We can load it up in a pandas dataframe: . ratings = pd.read_csv(path/&#39;ratings.csv&#39;) ratings . user_id book_id rating . 0 1 | 258 | 5 | . 1 2 | 4081 | 4 | . 2 2 | 260 | 5 | . 3 2 | 9296 | 5 | . 4 2 | 2318 | 3 | . ... ... | ... | ... | . 5976474 49925 | 510 | 5 | . 5976475 49925 | 528 | 4 | . 5976476 49925 | 722 | 4 | . 5976477 49925 | 949 | 5 | . 5976478 49925 | 1023 | 4 | . 5976479 rows × 3 columns . We have a user_id column that represents each user, book_id to represent the books and the rating column is the rating given to a book by a particular user. It ranges from 0 to 5. . Subset of the Dataset . We don&#39;t want to work on the full dataset right away since it would take a long time to model and this would limit the number of experiments we can carry out. . Instead, first we will work with only the first 100, 000 ratings. . ratings = ratings.iloc[:100000] . We check for any missing value and drop them if they exists: . ratings.isnull().values.any() . False . Another issue is with the book_id. The model as is would work even if we used book_id but for visual purposes, we&#39;d rather have the book title to look at, since that is more informative to us (not the model). . So we will have to load in the books.csv file and merge it with our ratings DataFrame. . books = pd.read_csv(path/&#39;books.csv&#39;) . books.columns . Index([&#39;book_id&#39;, &#39;goodreads_book_id&#39;, &#39;best_book_id&#39;, &#39;work_id&#39;, &#39;books_count&#39;, &#39;isbn&#39;, &#39;isbn13&#39;, &#39;authors&#39;, &#39;original_publication_year&#39;, &#39;original_title&#39;, &#39;title&#39;, &#39;language_code&#39;, &#39;average_rating&#39;, &#39;ratings_count&#39;, &#39;work_ratings_count&#39;, &#39;work_text_reviews_count&#39;, &#39;ratings_1&#39;, &#39;ratings_2&#39;, &#39;ratings_3&#39;, &#39;ratings_4&#39;, &#39;ratings_5&#39;, &#39;image_url&#39;, &#39;small_image_url&#39;], dtype=&#39;object&#39;) . We drop some columns which we don&#39;t require: . books.drop([&#39;best_book_id&#39;, &#39;work_id&#39;, &#39;title&#39;,&#39;books_count&#39;, &#39;language_code&#39;, &#39;isbn13&#39;, &#39;isbn&#39;,&#39;ratings_count&#39;, &#39;work_ratings_count&#39;, &#39;work_text_reviews_count&#39;, &#39;ratings_1&#39;, &#39;ratings_2&#39;, &#39;ratings_3&#39;, &#39;ratings_4&#39;, &#39;ratings_5&#39;,&#39;image_url&#39;, &#39;small_image_url&#39;], axis=1, inplace=True) books.head() . book_id goodreads_book_id authors original_publication_year original_title average_rating . 0 1 | 2767052 | Suzanne Collins | 2008.0 | The Hunger Games | 4.34 | . 1 2 | 3 | J.K. Rowling, Mary GrandPré | 1997.0 | Harry Potter and the Philosopher&#39;s Stone | 4.44 | . 2 3 | 41865 | Stephenie Meyer | 2005.0 | Twilight | 3.57 | . 3 4 | 2657 | Harper Lee | 1960.0 | To Kill a Mockingbird | 4.25 | . 4 5 | 4671 | F. Scott Fitzgerald | 1925.0 | The Great Gatsby | 3.89 | . Drop any missing values: . books.isnull().values.any() . True . books.dropna(inplace=True) . And finally merge it to one DataFrame: . ratings = ratings.merge(books) ratings.head() . user_id book_id rating goodreads_book_id authors original_publication_year original_title average_rating . 0 1 | 258 | 5 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 1 11 | 258 | 3 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 2 143 | 258 | 4 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 3 242 | 258 | 5 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 4 325 | 258 | 4 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . Creating the DataLoaders . Since that looks okay, we can go ahead and create our DataLoaders object that we will feed to the model. A DataLoaders is simply a convinient object that holds our training and validation data and will pass in the data to our model in mini-batches. . We can look up the documentation of a function by calling the doc function. . doc(CollabDataLoaders.from_df) . CollabDataLoaders.from_df[source] . CollabDataLoaders.from_df(ratings, valid_pct=0.2, user_name=None, item_name=None, rating_name=None, seed=None, path=&#39;.&#39;, bs=64, val_bs=None, shuffle_train=True, device=None) . Create a DataLoaders suitable for collaborative filtering from ratings. . Show in docs . The above tells us what we will need to pass in the function in order to create our DataLoaders. Specifically we will need to pass in : . The DataFrame we are creating from (ratings) | Size of the Validation split (20%) | The column that represents our user (user_id) | The column that represents our item (original_title) | The column that represents our rating (rating) | . dls = CollabDataLoaders.from_df(ratings=ratings, valid_pct=0.2, user_name=&#39;user_id&#39;, item_name=&#39;original_title&#39;, rating_name=&#39;rating&#39;) . We can look at one batch of data: . dls.show_batch() . user_id original_title rating . 0 791 | Sphere | 3 | . 1 543 | Metamorphoses | 5 | . 2 1835 | 1491: New Revelations of the Americas Before Columbus | 4 | . 3 2688 | Good Omens: The Nice and Accurate Prophecies of Agnes Nutter, Witch | 5 | . 4 264 | Jonathan Livingston Seagull | 5 | . 5 2074 | The Curious Incident of the Dog in the Night-Time | 4 | . 6 2136 | The Lovely Bones | 4 | . 7 1089 | Franny and Zooey | 5 | . 8 2334 | Great Expectations | 4 | . 9 750 | The History of the Hobbit, Part One: Mr. Baggins | 2 | . For the latent factors of both our books, we will initialize a random Embedding with a size of all the books we have and the number of factors we want. The same goes for the latent factors of our users. . The values in these Embeddings will be updated by BackPropagation until they become meaningful. . For our small subset of data, we will use 50 factors. This simply means we are giving our model an allowance to represent each books in 50 different ways (e.g. action, classics etc). . So, we first should get the number of users and the number of books: . n_books = len(dls.classes[&#39;original_title&#39;]) n_users = len(dls.classes[&#39;user_id&#39;]) . For learning purposes, we should also get into the habit of manually chekcking shapes of the data in our batches and see if we understand why it is so: . xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([64, 2]), torch.Size([64, 1])) . We are using a batch size of 64, so that makes sense. For our independent variable (X), we have two items there, which are the user and the book and for the dependent variable (y), we only have the rating of the book. That is why the above shapes are so. . Creating the Model . Next, we need to handle the model part. . class CollabFiltering(Module): def __init__(self, n_users, n_books, n_factors, y_range=(0, 5.5)): self.u_weights = Embedding(n_users, n_factors) self.u_bias = Embedding(n_users, 1) self.i_weights = Embedding(n_books, n_factors) self.i_bias = Embedding(n_books, 1) self.y_range = y_range def forward(self, x): users = self.u_weights(x[:,0]) books = self.i_weights(x[:,1]) res = (users * books).sum(dim=1, keepdims=True) res += self.u_bias(x[:,0]) + self.i_bias(x[:,1]) res = sigmoid_range(res, *self.y_range) return res . Our Model Inherits from Module which is what PyTorch expects: . class CollabFiltering(Module): . The model takes in the number of users, the number of books, the number of factors we want to use for modelling and a y_range that defaults to a tuple of (0, 5.5). . The purpose of the y_range is to squish our output between 0 and 5. (We choose 5.5 as the upper limit since we are going to be using sigmoid range to contain our output. And since values in sigmoid never reach the upper limit, to predict a value of 5, we are going to need to go higher). . It then creates the Embeddings of the users and the books. We are also going to create biases for each user and each book to capture the fact that some users generally more positive or negative in their recommendations than others and smoe books are just plain better or worse than others. . def __init__(self, n_users, n_books, n_factors, y_range=(0, 5.5)): self.u_weights = Embedding(n_users, n_factors) self.u_bias = Embedding(n_users, 1) self.i_weights = Embedding(n_books, n_factors) self.i_bias = Embedding(n_books, 1) self.y_range = y_range . The forward method simply gets the factors from the user embedding and book embedding, performs dot product on them, adds the biases then uses sigmoid range to squish the output between 0 and 5 and returns the result. . def forward(self, x): users = self.u_weights(x[:,0]) books = self.i_weights(x[:,1]) res = (users * books).sum(dim=1, keepdims=True) res += self.u_bias(x[:,0]) + self.i_bias(x[:,1]) res = sigmoid_range(res, *self.y_range) return res . We can now instantiate the model with 50 factors and create our Learner. We are going to use Mean Squared Error as our loss function since this is a regression problem. . model = CollabFiltering(n_users, n_books, 50) . learn = Learner(dls, model, loss_func=MSELossFlat()) . Let us find a suitable Learning rate and train the model for 6 epochs. We use a weight decay of 0.2 to regularize our model . learn.lr_find() . SuggestedLRs(lr_min=0.03019951581954956, lr_steep=7.585775892948732e-05) . learn.fit_one_cycle(6, 1e-2, wd=0.2) . epoch train_loss valid_loss time . 0 | 0.972651 | 0.988126 | 00:08 | . 1 | 0.959149 | 0.947992 | 00:08 | . 2 | 0.908314 | 0.913695 | 00:08 | . 3 | 0.789891 | 0.863736 | 00:08 | . 4 | 0.646984 | 0.828928 | 00:08 | . 5 | 0.538308 | 0.826404 | 00:08 | . We get a final MSE of 0.826404 which is actually pretty good for out task at hand. . Interpreting the Embeddings . Let us start by interpretting the book biases. We could have easily ranked the books by the ratings and checked the last five items but bias tells us something more interesting. These books are books that, even if a user was well matched to the latent factors of the book (e.g action, comedy etc), they still didn&#39;t like the books and that is why the books have a low bias. In short, they are generally bad books. . These are the 5 books with the lowest bias from our model. . book_bias = learn.model.i_bias.weight.squeeze() idxs = book_bias.argsort()[:5] [dls.classes[&#39;original_title&#39;][i] for i in idxs] . [&#39;Trading Up&#39;, &#39;One Night @ The Call Center&#39;, &#39;Four Blondes&#39;, &#39;Lost&#39;, &#39;Eat That Frog!: 21 Great Ways to Stop Procrastinating and Get More Done in Less Time&#39;] . Since our rating DataFrame included the GoodReads ID of each book, we can create a URL of a single book and check out the reviews from GoodReads. . Let&#39;s use Four Blondes as an example. We can locate its entry in our DataFrame using the following code: . books.loc[books[&#39;original_title&#39;]==&#39;Four Blondes&#39;] . book_id goodreads_book_id authors original_publication_year original_title average_rating . 4008 4009 | 6613 | Candace Bushnell | 2000.0 | Four Blondes | 2.8 | . Then using its goodreads_id, create a URL that we can follow to read its reviews: . id = 6613 url = f&#39;https://www.goodreads.com/book/show/{id}&#39; url . &#39;https://www.goodreads.com/book/show/6613&#39; . I haven&#39;t read the book myself but I don&#39;t think I will too. Reading the reviews from GoodReads looks like people didn&#39;t really enjoy this book. Here is one extract of the reviews: . . Now let us do the opposite, and check for the books with the highest book bias. This means that these are generally good books and even if you don&#39;t enjoy such genres, there is a high chance you will enjoy this books: . idxs = book_bias.argsort(descending=True)[:5] [dls.classes[&#39;original_title&#39;][i] for i in idxs] . [&#39;Harry Potter and the Deathly Hallows&#39;, &#39;A Thousand Splendid Suns&#39;, &#39;Where the Wild Things Are&#39;, &#39;Le Petit Prince&#39;, &#39;دیوان u200e u200e [Dīvān]&#39;] . At the very top is a book I enjoyed very much myself: Harry Potter and the Deathly Hallows. Since I may be biased towards that book, let us check for reviews of a book I haven&#39;t read myself. . id = int(books.loc[books[&#39;original_title&#39;]==&#39;Le Petit Prince&#39;][&#39;goodreads_book_id&#39;].values) url = f&#39;https://www.goodreads.com/book/show/{id}&#39; url . &#39;https://www.goodreads.com/book/show/157993&#39; . I was suprised to find that &#39;Le Petit Prince&#39; or &#39;The Little Prince&#39; was a child-like novel which adults enjoy as well. Here is one review from GoodReads: . . It is definitely on my to-read list now. . Next, let us try and intepret the Book Embedding Factors. Remember, we had 50 factors in our model. Well, intepreting a 50-dimensional Embedding turns out to be a difficult task for us human beings. . Luckily we have a method called PCA which stands for Principal Component Analysis that can be used to reduce the dimensions to something that can be plotted in a graph. . Here is the outcome: . #caption Representation of Books based on two strongest PCA components g = ratings.groupby(&#39;original_title&#39;)[&#39;rating&#39;].count() top_books = g.sort_values(ascending=False).index.values[:10000] top_idxs = tensor([learn.dls.classes[&#39;original_title&#39;].o2i[m] for m in top_books]) books_w = learn.model.i_weights.weight[top_idxs].cpu().detach() books_pca = books_w.pca(3) fac0,fac1,fac2 = books_pca.t() idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_books[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . As you can see, our model has learned for itself to seperate the books into different categories. If you look at the very top, we have the classic books, the likes of To Kill A MockingBird and The Great Gatsby. At the right side we have the likes of The Da Vinci Code and Angels and Demons which are more of Mystery Thriller Novels and are more of reality fiction novels. At the bottom we have The Hobit books which are more of fantasy novels and Mythology. . What is magical is our model learned all of these on its own! We never explicitely told it what genres a particular book belongs to. We only fed it user ratings and it learned all about them itself. . Generating Book Reviews . Now comes the interesting part. How to generate a book review for a user. It is actually a very simple concept. The intuition is two similar books should have a smaller distance between them (of the Embeddings) than two different books. So, we will require a reader to input the name of a book he or she enjoyed reading and we will generate a recommendation of books for them. . Let us experiment with &#39;The Da Vinci Code&#39; by Dan Brown and see the 5 most similar books to it using our model: . book_factors = learn.model.i_weights.weight idx = dls.classes[&#39;original_title&#39;].o2i[&#39;The Da Vinci Code&#39;] distances = nn.CosineSimilarity(dim=1)(book_factors, book_factors[idx][None]) idx = distances.argsort(descending=True)[1:6] # dls.classes[&#39;original_title&#39;][idx] [dls.classes[&#39;original_title&#39;][i] for i in idx] . [&#39;The Sweet Far Thing&#39;, &#39;Angels &amp; Demons &#39;, &#39;Back Roads&#39;, &#34;What to Expect When You&#39;re Expecting&#34;, &#39;What to Expect the First Year&#39;] . According to our model, The Sweet Far Thing is the most similar book, so let us generate a URL and see the reviews: . id = int(books.loc[books[&#39;original_title&#39;]==&#39;The Sweet Far Thing&#39;][&#39;goodreads_book_id&#39;].values) url = f&#39;https://www.goodreads.com/book/show/{id}&#39; url . &#39;https://www.goodreads.com/book/show/127459&#39; . The book description describes it as a Thriller novel which was the same case with The Da Vinci Code. . Neural Network . Now, we will experiment with a Neural Network Model and see if it performs better than our Dot Product Model. . To turn our Model into a Deep Learning Model, we need to concatenate the Embeddings together and pass them through linear layers with non-linearities between them. Our Neural network will have 100 neurons and utilize a ReLU between them. Here is the code: . class CollabFilteringNN(Module): def __init__(self, user_sz, book_sz, y_range=(0, 5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.book_factors = Embedding(*book_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+book_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1) ) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]), self.book_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . We then instantiate the neural network. We will use the get_emb_sz to get good Embedding sizes for our users and books. Remember since we will concatenate the Embeddings and not multiply them, they can now have different sizes. . nn_model = CollabFilteringNN(*get_emb_sz(dls)) . We can check out our model: . nn_model . CollabFilteringNN( (user_factors): Embedding(1795, 106) (book_factors): Embedding(4879, 186) (layers): Sequential( (0): Linear(in_features=292, out_features=100, bias=True) (1): ReLU() (2): Linear(in_features=100, out_features=1, bias=True) ) ) . We will now create our Learner with the same MSE loss function, find a suitable learning rate and fit our model for 5 epochs using a regularization weight decay of 0.05: . learn = Learner(dls, nn_model, loss_func=MSELossFlat()) . learn.lr_find() . SuggestedLRs(lr_min=0.006918309628963471, lr_steep=3.311311274956097e-06) . learn = Learner(dls, nn_model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 1e-2, wd=0.05) . epoch train_loss valid_loss time . 0 | 0.775485 | 0.819169 | 00:25 | . 1 | 0.805927 | 0.817436 | 00:29 | . 2 | 0.777418 | 0.803896 | 00:30 | . 3 | 0.716998 | 0.798450 | 00:26 | . 4 | 0.652017 | 0.812329 | 00:25 | . We get roughly the same final MSE. . Use Full DataSet . Since we are done prototyping and experimenting with the small dataset, it is time to use the full dataset for modelling. . Our full dataset has around 6 million ratings, instead of the 100k we worked with earlier. Therefore it will probably take longer to train. Due to that, we need to train the model for a short number of epochs then save the model&#39;s current progress before we continue training. . We will load in the ratings and pre-process the DataSet as before, then create a DataLoaders object and train the model: . ratings = pd.read_csv(path/&#39;ratings.csv&#39;) ratings.head() . user_id book_id rating . 0 1 | 258 | 5 | . 1 2 | 4081 | 4 | . 2 2 | 260 | 5 | . 3 2 | 9296 | 5 | . 4 2 | 2318 | 3 | . ratings = ratings.merge(books) ratings.head() . user_id book_id rating goodreads_book_id authors original_publication_year original_title average_rating . 0 1 | 258 | 5 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 1 11 | 258 | 3 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 2 143 | 258 | 4 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 3 242 | 258 | 5 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . 4 325 | 258 | 4 | 1232 | Carlos Ruiz Zafón, Lucia Graves | 2001.0 | La sombra del viento | 4.24 | . dls = CollabDataLoaders.from_df(ratings=ratings, valid_pct=0.2, user_name=&#39;user_id&#39;, item_name=&#39;original_title&#39;, rating_name=&#39;rating&#39;) . dls.show_batch() . user_id original_title rating . 0 14055 | The Crying of Lot 49 | 3 | . 1 52968 | The Gunslinger | 5 | . 2 14644 | The Five People You Meet in Heaven | 4 | . 3 5431 | The Gods Themselves | 5 | . 4 31889 | A Wrinkle in Time | 5 | . 5 27179 | Jurassic Park | 5 | . 6 46433 | Watchmen | 5 | . 7 43269 | The Fault in Our Stars | 1 | . 8 39282 | A Return to Love: Reflections on the Principles of &quot;A Course in Miracles&quot; | 5 | . 9 11652 | The Mist | 3 | . n_users = len(dls.classes[&#39;user_id&#39;]) n_books = len(dls.classes[&#39;original_title&#39;]) . doc(collab_learner) . collab_learner[source] . collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, opt_func=Adam, lr=0.001, splitter=trainable_params, cbs=None, metrics=None, path=None, model_dir=&#39;models&#39;, wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95)) . Create a Learner for collaborative filtering on dls. . Show in docs . We will use the collab_learner provided by fastai for convinience purposes and since I want to export my model after and create an inference app after. . Note: It does the exact same thing as our DotProduct Model but has some nice additions on top. . learn = collab_learner(dls, n_factors=500, y_range=(0, 5.5)) . learn.lr_find() . SuggestedLRs(lr_min=0.19054607152938843, lr_steep=2.511886486900039e-05) . learn.fit_one_cycle(4, 1e-1) . epoch train_loss valid_loss time . 0 | 7.333699 | 7.176844 | 24:07 | . 1 | 7.123693 | 6.995453 | 24:09 | . 2 | 5.130961 | 5.162658 | 23:57 | . 3 | 1.104243 | 1.095618 | 24:06 | . After 4 epochs, we get a MSE of 1.09. This is probably a good time to save the model so we do not lose our progress: . learn.save(&#39;4epochs&#39;) . Path(&#39;models/4epochs.pth&#39;) . We can now load up the model and find continue training it for two more epochs and see if it improves: . learn = learn.load(&#39;4epochs&#39;) . learn.model . EmbeddingDotBias( (u_weight): Embedding(53425, 500) (i_weight): Embedding(9269, 500) (u_bias): Embedding(53425, 1) (i_bias): Embedding(9269, 1) ) . learn.lr_find() . SuggestedLRs(lr_min=0.002754228748381138, lr_steep=8.31763736641733e-06) . learn.fit_one_cycle(2, 1e-3) . epoch train_loss valid_loss time . 0 | 0.631629 | 0.749671 | 24:14 | . 1 | 0.374357 | 0.715882 | 24:15 | . We go down to an MSE of 0.71 which is even better than what we got with the subset of the dataset. . Since it is taking long to train and I was happy with the results at this point, I stopped after 6 epochs and saved the model. However, if you have time and resources, you can continue training using the same process of saving progress after a few epochs. . learn.save(&#39;6epochs&#39;) . Path(&#39;models/6epochs.pth&#39;) . Interpreting the Full Model . We can see what our model with the full dataset learned. . We will start with the 5 books with the lowest bias (Generally bad books even if you would like the genre): . book_bias = learn.model.i_bias.weight.squeeze() idxs = book_bias.argsort()[:5] [dls.classes[&#39;original_title&#39;][i] for i in idxs] . [&#39;One Night @ The Call Center&#39;, &#39;Half Girlfriend&#39;, &#39;ليتها تقرأ&#39;, &#39;The 3 Mistakes of My Life&#39;, &#39;of course i love you&#39;] . The book One Night @ The Call Center makes another appearance. It must be a really bad book. . Now for the generally good books, the books with the highest book bias: . idxs = book_bias.argsort(descending=True)[:5] [dls.classes[&#39;original_title&#39;][i] for i in idxs] . [&#39;The Complete Calvin and Hobbes&#39;, &#39;Words of Radiance&#39;, &#39;Complete Harry Potter Boxed Set&#39;, &#39;The Essential Calvin and Hobbes: A Calvin and Hobbes Treasury&#39;, &#39;The Calvin and Hobbes Tenth Anniversary Book&#39;] . Harry Potter makes another appearance. Let us check out one of the books: . id = int(books.loc[books[&#39;original_title&#39;]==&#39;Words of Radiance&#39;][&#39;goodreads_book_id&#39;].values) url = f&#39;https://www.goodreads.com/book/show/{id}&#39; url . &#39;https://www.goodreads.com/book/show/17332218&#39; . Here is a sample review of the book: . . Next we can check out the Book Embeddings using the PCA trick we utilized before: . learn.model . EmbeddingDotBias( (u_weight): Embedding(53425, 500) (i_weight): Embedding(9269, 500) (u_bias): Embedding(53425, 1) (i_bias): Embedding(9269, 1) ) . #caption Representation of books based on two strongest PCA components g = ratings.groupby(&#39;original_title&#39;)[&#39;rating&#39;].count() top_books = g.sort_values(ascending=False).index.values[:10000] top_idxs = tensor([learn.dls.classes[&#39;original_title&#39;].o2i[m] for m in top_books]) books_w = learn.model.i_weight.weight[top_idxs].cpu().detach() books_pca = books_w.pca(3) fac0,fac1,fac2 = books_pca.t() idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_books[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . And we can check the distances (similar books). We use The Da Vinci Code again as an example: . book_factors = learn.model.i_weight.weight idx = dls.classes[&#39;original_title&#39;].o2i[&#39;The Da Vinci Code&#39;] distances = nn.CosineSimilarity(dim=1)(book_factors, book_factors[idx][None]) idx = distances.argsort(descending=True)[1:6] # dls.classes[&#39;original_title&#39;][idx] [dls.classes[&#39;original_title&#39;][i] for i in idx] . [&#39;Angels &amp; Demons &#39;, &#39;The Lost Symbol&#39;, &#39;Deception Point&#39;, &#39;Digital Fortress&#39;, &#39;The Firm&#39;] . Now that I am happy with my model, I will export it and use it to create a Web App for Recommending Similar Books. That will be in Part 2 of this blog and it is coming soon. . learn.export(&#39;final_model.pkl&#39;) .",
            "url": "https://jimmiemunyi.github.io/blog/projects/tutorial/2021/02/15/Book-Recommendation-Model-Training.html",
            "relUrl": "/projects/tutorial/2021/02/15/Book-Recommendation-Model-Training.html",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Real Time Sign Language Classification",
            "content": "Introduction . After training our model in Part A, we are now going to develop an application to run inference with for new data. . I am going to be utilizing opencv to get live video from my webcam, then run our model against each frame in the video and get the prediction of what Sign Language Letter I am holding up. . Here is an example of what the output will look like: . The whole code + training notebooks from Part A can be found in this github repo. . This tutorial assumes some basic understanding of the cv2 library and general understanding of how to run inference using a model. . The Full Code . Here is the full code of making the App if you just want the code. . I will explain each part of the code and what was my thinkinh behind it in the next section. . from collections import deque, Counter import cv2 from fastai.vision.all import * print(&#39;Loading our Inference model...&#39;) # load our inference model inf_model = load_learner(&#39;model/sign_language.pkl&#39;) print(&#39;Model Loaded&#39;) # define a deque to get rolling average of predictions # I go with the last 10 predictions rolling_predictions = deque([], maxlen=10) # get the most common item in the deque def most_common(D): data = Counter(D) return data.most_common(1)[0][0] def hand_area(img): # specify where hand should go hand = img[50:324, 50:324] # the images in the model were trainind on 200x200 pixels hand = cv2.resize(hand, (200,200)) return hand # capture video on the webcam cap = cv2.VideoCapture(0) # get the dimensions on the frame frame_width = int(cap.get(3)) frame_height = int(cap.get(4)) # define codec and create our VideoWriter to save the video fourcc = cv2.VideoWriter_fourcc(*&#39;mp4v&#39;) out = cv2.VideoWriter(&#39;output/sign-language.mp4&#39;, fourcc, 12, (frame_width, frame_height)) # read video while True: # capture each frame of the video ret, frame = cap.read() # flip frame to feel more &#39;natural&#39; to webcam frame = cv2.flip(frame, flipCode = 1) # draw a blue rectangle where to place hand cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2) # get the image inference_image = hand_area(frame) # get the current prediction on the hand pred = inf_model.predict(inference_image) # append the current prediction to our rolling predictions rolling_predictions.append(pred[0]) # our prediction is going to be the most common letter # in our rolling predictions prediction_output = f&#39;The predicted letter is {most_common(rolling_predictions)}&#39; # show predicted text cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2) # show the frame cv2.imshow(&#39;frame&#39;, frame) # save the frames to out file out.write(frame) # press `q` to exit if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;): break # release VideoCapture() cap.release() # release out file out.release() # close all frames and video windows cv2.destroyAllWindows() . Explaining the Code . Imports . Install fastai and opencv-python. . Next, this are the packages I utilize for this App. fastai is going to be used to run Inference with, cv2 is going to handle all the WebCam functionality and we are going to utilize deque and Counter from collections to apply a nifty trick I am going to show you. . from collections import deque, Counter import cv2 from fastai.vision.all import * . Loading our Inference Model . print(&#39;Loading our Inference model...&#39;) # load our inference model inf_model = load_learner(&#39;model/sign_language.pkl&#39;) print(&#39;Model Loaded&#39;) . The next part of our code loads the model we pickled in Part A and prints some useful information. . Rolling Average Predictions . When I first made the App, I noticed one problem when using it. A slight movement of my hand was changing the predictions. This is known as flickering. The video below shows how flickering affects our App: . . The Video you saw in the beginning shows how &#39;stable&#39; our model is after using rolling predictions. . # define a deque to get rolling average of predictions # I go with the last 10 predictions rolling_predictions = deque([], maxlen=10) # get the most common item in the deque def most_common(D): data = Counter(D) return data.most_common(1)[0][0] . To solve this, a utilized the deque from Collections. I used 10 as the maxlength of the deque since I wanted the App, when running inference, to output the most common prediction out of the last 10 predictions. This makes it more stable than when we are using only the current one. . The function most_common will return the most common item in our deque. . Hand Area . def hand_area(img): # specify where hand should go hand = img[50:324, 50:324] # the images in the model were trainind on 200x200 pixels hand = cv2.resize(hand, (200,200)) return hand . Next, we define a function that tells our model which part of the video to run inference on. We do not want to run inference on the whole video which will include our face! We will eventually draw a blue rectangle in this area so that you&#39;ll know where to place your hand. . Capture Video on the WebCam and Define Our Writer . # capture video on the webcam cap = cv2.VideoCapture(0) # get the dimensions on the frame frame_width = int(cap.get(3)) frame_height = int(cap.get(4)) # define codec and create our VideoWriter to save the video fourcc = cv2.VideoWriter_fourcc(*&#39;mp4v&#39;) out = cv2.VideoWriter(&#39;sign-language.mp4&#39;, fourcc, 12, (frame_width, frame_height)) . Here, we define a VideoCapture that will record our video. The parameter 0 means capture on the first WebCam it finds. If you have multiple WebCams, this is the parameter you want to play around with until you find the correct one. . Next, we get the dimensions of the frame being recorded by the VideoCapture. We are going to use this dimensions when writing (outputting) the recorded video . Finally, we create a VideoWriter that we are going to use to output the video and write it to our Hard Disk. To do that, opencv requires us to define a codec to use, and so we create a VideoWriter_fourcc exactly for that purpose and we use &#39;mp4v&#39; with it. . In our writer, we first pass the name we want for the output file, here I use &#39;sign-language.mp4&#39; which will be written in the current directory. You can change this location if you wish to. Next we pass in the codec. After that you pass in your fps (frame rate per second). I found that 12 worked best with my configuration but you probably want to play around with that until you get the best one for you. Finally, we pass in the frame sizes, which we had gotten earlier. . The Main Video Loop . # read video while True: # capture each frame of the video ret, frame = cap.read() # flip frame to feel more &#39;natural&#39; to webcam frame = cv2.flip(frame, flipCode = 1) # draw a blue rectangle where to place hand cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2) # get the image inference_image = hand_area(frame) # get the current prediction on the hand pred = inf_model.predict(inference_image) # append the current prediction to our rolling predictions rolling_predictions.append(pred[0]) # our prediction is going to be the most common letter # in our rolling predictions prediction_output = f&#39;The predicted letter is {most_common(rolling_predictions)}&#39; # show predicted text cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2) # show the frame cv2.imshow(&#39;frame&#39;, frame) # save the frames to out file out.write(frame) # press `q` to exit if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;): break . This is a long piece of code so lets break it down bit by bit: . # read video while True: # capture each frame of the video _ , frame = cap.read() # flip frame to feel more &#39;natural&#39; to webcam frame = cv2.flip(frame, flipCode = 1) # ...... # truncated code here # ...... # show the frame cv2.imshow(&#39;frame&#39;, frame) # save the frames to out file out.write(frame) # press `q` to exit if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;): break . We create a infinite While loop that will always be running, until the user presses the &#39;q&#39; letter on the keyboard, as defined by our last if statement at the very bottom of the loop. . After that, we use the reader we created earlier and call cap.read() on it which returns the current frame of the video, and another variable that we are not going to use. . A little intuition how videos works. A frame is somewhat equivalent to just one static image. Think of it as that. So for a video what usually happens it these single frames are played one after the other quickly, like 30-60 times faster hence creating the illusion of a continious video. . So for our App, we are going to get each frame, and run it through our model (which expects the input to be an image, so this will work) and return the current prediction. This is also why we decided to use rolling average predictions and not the just the current prediction. To reduce the flickering that may occur by passing a different frame each second. . Next: . frame = cv2.flip(frame, flipCode = 1) . This flips our frame to make it feel more natural. What I mean is, without flipping, the output image felt reversed, where if I raised my left arm it seemed like I was raising my right. Try running the App with this part commented out and you&#39;ll get what I mean. . This shows the frames one after the other and the out writes it to disk . cv2.imshow(&#39;frame&#39;, frame) # save the frames to out file out.write(frame) . # read video while True: # ...... # truncated code here # ...... # draw a blue rectangle where to place hand cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2) # get the image inference_image = hand_area(frame) # get the current prediction on the hand pred = inf_model.predict(inference_image) # append the current prediction to our rolling predictions rolling_predictions.append(pred[0]) # our prediction is going to be the most common letter # in our rolling predictions prediction_output = f&#39;The predicted letter is {most_common(rolling_predictions)}&#39; # show predicted text cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2) # ...... # truncated code here # ...... # press `q` to exit if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;): break . Next, we draw a blue rectangle where the user should place the hand. The first parameter is where we want to draw the rectangle and we tell opencv to draw it on our current frame. The next two parameter describe the area where we want our rectangle to be. Note that this dimensions are exactly the same as those in the hand_area function we created earlier. This is to make sure we are running inference on the correct area. Lastly we pass in the color of the rectangle (in BGR formart) and the thickness of the line (2). . cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2) . Next, from our whole frame, we just extract the hand area and store it. This is the image we are going to pass to our model . inference_image = hand_area(frame) . Next, we pass our extracted image to our inference model and get the predictions and append that prediction to our rolling predictions deque. Remember that this deque will only hold the most recent 10 predictions and discard everything else . pred = inf_model.predict(inference_image) rolling_predictions.append(pred[0]) . We get the most common Letter predicted in our Deque and use opencv to write that letter to the video. The parameters are almost similar to the rectangle code, with a slight variation since here we have to pass in the font(hershey simplex) and font size (0.9) . prediction_output = f&#39;The predicted letter is {most_common(rolling_predictions)}&#39; cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2) . The final part of the code just releases the resources we had acquired initially: the Video reader, the Video Writer and then destroys all windows created. . # release VideoCapture() cap.release() # release out file out.release() # close all frames and video windows cv2.destroyAllWindows() . And that&#39;s all in Part B. Hope you enjoyed it. .",
            "url": "https://jimmiemunyi.github.io/blog/projects/tutorial/2021/01/21/Sign-Language-Inference-with-WebCam.html",
            "relUrl": "/projects/tutorial/2021/01/21/Sign-Language-Inference-with-WebCam.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Sign Language Classification - Part A",
            "content": "Introduction . I am going to attempt to use Deep Learning to create a model that can learn the American Sign Language. For this Part A, we will focus on model training and for Part B, we are going to create an application from our model that we get here. . We are going to utilize Transfer Learning for this project, which is an important part of Deep Learning. . While I do not claim that this will be the best application out there for this particular problem, this small project could serve as motivation and can be expanded in future to create products that help the affected people who must use sign language to communicate. . Importing Packages . We are going to be using fastai so let&#39;s import it: . from fastai.vision.all import * . The Data . The dataset we are going to be using is American Sign Language Dataset from Kaggle. It contains 87,000 images each of 200x200 pixels. It has 29 classes: 26 for the letters A-Z and 3 classes for space, delete and nothing. We are going to use the Kaggle API to get the data. . !kaggle datasets download -d grassknoted/asl-alphabet . Downloading asl-alphabet.zip to /content 100% 1.03G/1.03G [00:05&lt;00:00, 222MB/s] . Let&#39;s unzip the data and get rid of the zip file: . #collapse-output !unzip *zip -d data &amp;&amp; rm -rf *zip . We create a Pathlib object pointing to our data folder and look inside to see what it contains: . path = Path(&#39;data&#39;) Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;asl_alphabet_test&#39;),Path(&#39;asl_alphabet_train&#39;)] . Let&#39;s peek into one of those folders: . (path/&#39;asl_alphabet_train&#39;).ls() . (#29) [Path(&#39;asl_alphabet_train/X&#39;),Path(&#39;asl_alphabet_train/G&#39;),Path(&#39;asl_alphabet_train/V&#39;),Path(&#39;asl_alphabet_train/I&#39;),Path(&#39;asl_alphabet_train/space&#39;),Path(&#39;asl_alphabet_train/N&#39;),Path(&#39;asl_alphabet_train/W&#39;),Path(&#39;asl_alphabet_train/P&#39;),Path(&#39;asl_alphabet_train/H&#39;),Path(&#39;asl_alphabet_train/Z&#39;)...] . We have 29 folders, as explained earlier. . Data Preprocessing . Now we are ready to create a DataBlock blueprint to hold our data. We use the fastai DataBlock API which is a convenient way to define how to handle our data. . Since we don&#39;t have validation data provided, we will split 20% of the training images and use it as our validation data. . We then create a DataLoaders object from the DataBlock, we will use a batch-size of 64. . signs = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42, valid_pct=0.2), item_tfms=Resize(200), batch_tfms=aug_transforms() ) dls = signs.dataloaders(path/&#39;asl_alphabet_train&#39;, bs=64) . Let&#39;s look into one batch of the data: . dls.show_batch() . This is the number of steps we are going to take in an epoch: . len(dls.train) . 1087 . Using Transfer Learning to Create A Model . Now we can create a model and use Transfer Learning to train it on our data. Transfer Learning is important since it enables us to get good results with less training and data. . For those who wish to replicate this experiment, we use: resnet18 architecture, Cross Entropy Loss since this is a Classification Task, and for our optimizer, we select the Adam Optimizer. We will output error rate and accuracy as our metrics to help as analyze how our model is doing. . We use the Learning Rate Finder provided by fastai, using insights from Leslie Smith&#39;s work, that enable us to find us a good learning rate, in short time instead of us trying a couple of learning rates experimentally and seeing what works. . If you are interested in reading more about the Learning Rate Finder, read this paper. . For our tast, it looks like a learning rate of 1x10-2 will work, so we fine-tine (transfer learn) for 4 epochs. . learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=[error_rate, accuracy], opt_func=Adam) learn.lr_find() . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . SuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.015848932787775993) . learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=[error_rate, accuracy], opt_func=Adam) learn.fine_tune(4, base_lr=1e-2) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.230984 | 0.065030 | 0.019310 | 0.980690 | 04:11 | . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.160366 | 0.875754 | 0.117989 | 0.882011 | 05:24 | . 1 | 0.038482 | 0.008822 | 0.002529 | 0.997471 | 05:26 | . 2 | 0.008333 | 0.000855 | 0.000230 | 0.999770 | 05:25 | . 3 | 0.002372 | 0.000346 | 0.000057 | 0.999943 | 05:25 | . We get a very good accuracy only after 4 epochs. . Now we can tackle the small test set that comes with this dataset, although we will scale up to a better test dataset in a few moments. . Testing Our Model . test_images = (path/&#39;asl_alphabet_test&#39;).ls() test_images . (#28) [Path(&#39;asl_alphabet_test/U_test.jpg&#39;),Path(&#39;asl_alphabet_test/space_test.jpg&#39;),Path(&#39;asl_alphabet_test/N_test.jpg&#39;),Path(&#39;asl_alphabet_test/R_test.jpg&#39;),Path(&#39;asl_alphabet_test/H_test.jpg&#39;),Path(&#39;asl_alphabet_test/P_test.jpg&#39;),Path(&#39;asl_alphabet_test/T_test.jpg&#39;),Path(&#39;asl_alphabet_test/C_test.jpg&#39;),Path(&#39;asl_alphabet_test/X_test.jpg&#39;),Path(&#39;asl_alphabet_test/V_test.jpg&#39;)...] . We have 28 images in this test set, each for the classes of data we have. . Let&#39;s take the first two images and predict them using our model. . U = (path/&#39;asl_alphabet_test&#39;/&#39;U_test.jpg&#39;) space = (path/&#39;asl_alphabet_test&#39;/&#39;space_test.jpg&#39;) . learn.predict(U)[0] . &#39;U&#39; . learn.predict(space)[0] . &#39;space&#39; . That looks like its working well. To predict on all the images in the test set, we are going to need a way to get the labels of the images, so as to compare with our prediction. . Let&#39;s work with one image first: . u_test = test_images[0] u_test . Path(&#39;asl_alphabet_test/U_test.jpg&#39;) . As you can see, the label of the test images is contained in the filename. So we are going to use regular expressions to extract the label from the filenames. . Here is a simple regular expression that does the job: . re.findall(&#39;(.+)_test.jpg$&#39;, u_test.name)[0] . &#39;U&#39; . And our prediction on that image: . learn.predict(u_test)[0] . &#39;U&#39; . And now, a way to compare our prediction, to the true label of the test set: . re.findall(&#39;(.+)_test.jpg$&#39;, u_test.name)[0] == learn.predict(u_test)[0] . True . Let us write a function that is going to extract the labels, and store them in a list: . def get_test_names(images): labels = [] for i in images: label = re.findall(&#39;(.+)_test.jpg$&#39;, i.name)[0] labels.append(label) return labels . We can now get all the labels for the 28 images in our test set: . test_labels = get_test_names(test_images) . len(test_labels) . 28 . Now we need a function to run inference on the images. It is going to take in the images, our model and the labels we just got as parameters and output the mean accuracy of our predictions: . def run_inference(images, model, labels): corrects = [] # get the number of images to inference num_images = len(images) for i in range(num_images): # get the inference for an image prediction = model.predict(images[i])[0] # compare with the label for that image is_equal = (prediction==labels[i]) # append result to the list corrects.append(is_equal) # convert the list of inferences to float Tensor corrects = torch.Tensor(corrects).float() # return the mean accuracy return corrects.mean().item() . We can use that function to get the mean accuracy of our model on the small test dataset. . test_accuracy = run_inference(test_images, learn, test_labels) test_accuracy . 1.0 . We get 100% accuracy. Impressive. But as I mentioned earlier, this is a small dataset, that doesn&#39;t do well to tell us how our model will generalize to new data. . Luckily, there is another dataset recommended to be used as a test set for this dataset. It contains 870 images, 30 images for each category. . Let us use the Kaggle API again to get this new dataset: . !kaggle datasets download -d danrasband/asl-alphabet-test . Downloading asl-alphabet-test.zip to /content 70% 17.0M/24.3M [00:00&lt;00:00, 24.5MB/s] 100% 24.3M/24.3M [00:00&lt;00:00, 33.3MB/s] . And unzip it to a test folder: . #collapse-output !unzip *zip -d test &amp;&amp; rm -rf *zip . test_path = Path(&#39;test&#39;) test_path.ls() . (#29) [Path(&#39;test/X&#39;),Path(&#39;test/G&#39;),Path(&#39;test/V&#39;),Path(&#39;test/I&#39;),Path(&#39;test/space&#39;),Path(&#39;test/N&#39;),Path(&#39;test/W&#39;),Path(&#39;test/P&#39;),Path(&#39;test/H&#39;),Path(&#39;test/Z&#39;)...] . We use the get_image_files to recursively get images from the newly-created test path. We get 870 images, so that seems to be working fine. . test_files = get_image_files(test_path) len(test_files) . 870 . To run inference, we are required to perform the same data preprocessing we perfomed on the training images. To make this easier, fastai suggest we use a test_dl that is created using the following syntax: . test_dl = learn.dls.test_dl(test_files, with_label=True) test_dl.show_batch() . We can now get the predicitons on all the test images easily using the get_preds function and store it in a test_preds variable. . test_preds = learn.get_preds(dl=test_dl) . This are our current vocabs of our data: . learn.dls.vocab . [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;, &#39;N&#39;, &#39;O&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;R&#39;, &#39;S&#39;, &#39;T&#39;, &#39;U&#39;, &#39;V&#39;, &#39;W&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;, &#39;del&#39;, &#39;nothing&#39;, &#39;space&#39;] . To extract the true labels from the test images, we are again going to turn to regular expressions. But this time, we are required to write a regular expression robust enough to handle this three cases that represent how the rest of the images are named: . example_name = test_dl.items[370].name example_name . &#39;del0002_test.jpg&#39; . re.findall(r&#39;([A-Za-z]+) d+_test.jpg$&#39;, example_name)[0] . &#39;del&#39; . example_name_2 = test_dl.items[690].name example_name_2 . &#39;nothing0013_test.jpg&#39; . re.findall(r&#39;([A-Za-z]+) d+_test.jpg$&#39;, example_name_2)[0] . &#39;nothing&#39; . example_name_3 = test_dl.items[0].name example_name_3 . &#39;X0023_test.jpg&#39; . re.findall(r&#39;([A-Za-z]+) d+_test.jpg$&#39;, example_name_3)[0] . &#39;X&#39; . Now that we have that robust expression, we can proceed to check the accuracy of our prediction that we calculated: . We create a list to hold the result of our comparisons, from the predictions and the true labels, which we are going to use to calculate the final accuracy. . We also create a category_corrects dictionary, to tally for us, for each category, how many we predicted correct, so that we can see how our model performs on each category individually. . # create a list to hold True or False when comparing corrects = [] # count how many predictions we get correct per category category_corrects = dict.fromkeys(learn.dls.vocab, 0) # for each enumerated predictions for index, item in enumerate(test_preds[0]): # get the predicted vocab prediction = learn.dls.categorize.decode(np.argmax(item)) # get the confidence of the prediction confidence = max(item) confidence_percent = float(confidence) # get the true label for the image we are predicting image_name = test_dl.items[index].name label = re.findall(r&#39;([A-Za-z]+) d+_test.jpg$&#39;, image_name)[0] # get the comparison and append it to our corrects list is_correct = (prediction==label) corrects.append(is_correct) # if we got the prediction correct for that category, # increase the count by one if is_correct: category_corrects[prediction] += 1 # convert the list of inferences to float Tensor corrects = torch.Tensor(corrects).float() # print the mean accuracy print(f&#39;Accuracy on the test set: {corrects.mean().item():.4f}&#39;) . Accuracy on the test set: 0.6195 . As you can see, using this better test set, we can see that the accuracy reduces. . Since this is out of domain data, my intuition is that a better dataset that varies more could be collected and used in future. However for now, let&#39;s work with these results. . Remember, the test set is used as a final measure, and we shouldn&#39;t use it to improve our model . Let us check on the per-category prediction: . category_corrects . {&#39;A&#39;: 15, &#39;B&#39;: 30, &#39;C&#39;: 17, &#39;D&#39;: 25, &#39;E&#39;: 19, &#39;F&#39;: 13, &#39;G&#39;: 13, &#39;H&#39;: 30, &#39;I&#39;: 29, &#39;J&#39;: 22, &#39;K&#39;: 21, &#39;L&#39;: 29, &#39;M&#39;: 22, &#39;N&#39;: 14, &#39;O&#39;: 19, &#39;P&#39;: 30, &#39;Q&#39;: 29, &#39;R&#39;: 17, &#39;S&#39;: 7, &#39;T&#39;: 0, &#39;U&#39;: 11, &#39;V&#39;: 0, &#39;W&#39;: 27, &#39;X&#39;: 4, &#39;Y&#39;: 28, &#39;Z&#39;: 11, &#39;del&#39;: 23, &#39;nothing&#39;: 14, &#39;space&#39;: 20} . A plot would be better to analyze the information: . fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(*zip(*category_corrects.items())) plt.show() . Our model performs really poorly on the letters T, V, and X!! We will see if this is going to be a problem in our Application that we create in part 2. . Making our Inference Model More Robust . Since I plan on using this model to create a Computer Vision Model, I deciced to retrain my model, adding the new dataset in order to make it more robust, since that data varied more. . NOTE: This is not how it should be done, you should never use your test set to train the model. I only combined the two datasets into one in order to get a better model since good Sign Language Data is hard to come by and the test set we used isn&#39;t the official test set, just a recommended dataset that could have been used for training a different model. Also, I didn&#39;t use my new model to get a better prediction on the test set. . signs = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(200), batch_tfms=aug_transforms() ) dls = signs.dataloaders(path/&#39;asl_alphabet_train&#39;, bs=64) . len(dls.train) . 1098 . learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=[error_rate, accuracy], opt_func=Adam) learn.fine_tune(4, base_lr=1e-2) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.248037 | 0.073583 | 0.020769 | 0.979231 | 04:13 | . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.121224 | 0.061025 | 0.016502 | 0.983498 | 05:27 | . 1 | 0.036989 | 0.008055 | 0.001878 | 0.998122 | 05:27 | . 2 | 0.007163 | 0.003641 | 0.001081 | 0.998919 | 05:27 | . 3 | 0.003785 | 0.002509 | 0.000569 | 0.999431 | 05:27 | . I can now export my model, which I will use in the next Part, to create a Computer Vision Model to predict new data. Stay tuned! . learn.export(&#39;sign_language.pkl&#39;) . Here is the link to Part B .",
            "url": "https://jimmiemunyi.github.io/blog/tutorial/2021/01/20/Sign-Language-Classification-with-Deep-Learning.html",
            "relUrl": "/tutorial/2021/01/20/Sign-Language-Classification-with-Deep-Learning.html",
            "date": " • Jan 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Pneumonia X-Ray Classification.",
            "content": ". Introduction . Recently, I have been widely interested in the overlap between Deep Learning and Biology and decided to start learning about it. I came across an interesting challenge, where I try to build a Pneumonia Binary Classification Computer Vision model that predicts whether a chest X-ray has Pneumonia or not. I also learned a nifty approach to deal with a problem that is common in Medical Datasets, that I will show you here. . I am going to be using fastai and PyTorch for this tutorial. I want to extend my thanks to the author of this dataset from Kaggle that we are going to be using today. . Let&#39;s get the packages that we will need: . from fastai.vision.all import * import matplotlib.pyplot as plt import seaborn as sns . How does our data look like? . path = Path(&#39;data/chest_xray&#39;) . path.ls() . (#5) [Path(&#39;__MACOSX&#39;),Path(&#39;chest_xray&#39;),Path(&#39;val&#39;),Path(&#39;train&#39;),Path(&#39;test&#39;)] . It is already separated for us in the relevant folders. Awesome! Let&#39;s check inside one of the folders: . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/PNEUMONIA&#39;),Path(&#39;train/NORMAL&#39;)] . The folders are also separated into their respective classes. How many images do we have per category? . train = get_image_files(path/&#39;train&#39;) val = get_image_files(path/&#39;val&#39;) test = get_image_files(path/&#39;test&#39;) print(f&quot;Train: {len(train)}, Valid: {len(val)}, Test: {len(test)}&quot;) . Train: 5216, Valid: 16, Test: 624 . Our validation set has only 16 images! That won&#39;t be a good measurement of how our model is performing but we will tackle that later on. . Let us check the distribution of images between the two classes: . normal = get_image_files(path/&#39;train&#39;/&#39;NORMAL&#39;) pneumonia = get_image_files(path/&#39;train&#39;/&#39;PNEUMONIA&#39;) print(f&quot;Normal Images: {len(normal)}. Pneumonia Images: {len(pneumonia)}&quot;) . Normal Images: 1341. Pneumonia Images: 3875 . data = [[&#39;Normal&#39;, len(normal)], [&#39;Pneumonia&#39;, len(pneumonia)]] df = pd.DataFrame(data, columns=[&#39;Class&#39;, &#39;Count&#39;]) sns.barplot(x=df[&#39;Class&#39;], y=df[&#39;Count&#39;]); . Remember the problem common to Medical Datasets I was talking about? We see that our dataset is imbalanced. Our negative class (Normal) is 3 times less than our positive class. This is a problem. How do we solve it? . First, we will utilize some Data Augmentations. This is artificially growing our dataset by introducing some transforms on the images. . Second, we will use some lessons that I read from a wonderful paper: Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks that studies the problem of class imbalances and offers a way to solve it. I highly recommend reading the paper. . However, we need to build a Baseline model that we can later improve on. . For the Data Augmentations, we have to be careful to pick the ones that make sense for our X-Ray data. I picked Rotate and Zoom. If you think about it, transforms like flipping the image won&#39;t be useful since our body parts are in specific locations. e.g our liver is one the right, and flipping the X-Ray would take it to the opposite side. . I also utilize a nifty trick called Presizing from the fastai team. The basic idea behind this approach is this: We first resize the image to a bigger size, bigger than what we want for the final image. For instance, here, I resize the image to 460x460 first, then later on, resize it to 224x224 and at the same time, apply all the augmentaions at once. That is the most important point, applying the final resize and the transforms at the same time, preferably as a batch transform on the GPU. This helps in a higher quality image than insted, let&#39;s say, applying them one by one, which may degrade the data. To learn more about presizing, check out his notebook. . augs = [RandomResizedCropGPU(size=224, min_scale=0.75), Rotate(), Zoom()] . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=GrandparentSplitter(train_name=&#39;train&#39;, valid_name=&#39;val&#39;), item_tfms=Resize(460), batch_tfms=augs ) . Let us collect the data in a dataloaders object and show one batch. . dls = dblock.dataloaders(path) . dls.show_batch() . We are going to utilize transfer learning on the resnet18 architecture. Our metrics to guide us are going to be error rate and accuracy. . learn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy]) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . We use another great tool by fastai that helps us get the optimal learning rate to use. Something around 1x10-2 will work okay according to the plot. (The bottom scale is Logarithmic) . learn.lr_find() . SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.0008317637839354575) . Let me explain, what happens in the next few cells. In Transfer Learning, we need to retain the knowledge learned by the pretrained model. So what happens is, we freeze all the earlier layers and chop off the last classification layer and replace it with a layer with random weights and the correct number outputs, two in this case(it is done by default in fastai when creating the learner through the cnn_learner method). . So, first we train the final layer (with random weights) for 3 epochs, with the one cycle training policy. Then we unfreeze the whole model, find the new suitable learning rate (because we are now updating all the weights) and train for a further 3 epochs. . learn.fit_one_cycle(3, lr_max=1e-2) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.168814 | 0.368405 | 0.187500 | 0.812500 | 03:13 | . 1 | 0.099094 | 0.734940 | 0.312500 | 0.687500 | 03:15 | . 2 | 0.063019 | 0.439835 | 0.187500 | 0.812500 | 03:17 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=2.2908675418875646e-06) . This plot looks different from the other one, since we are now updating all the weights, not just the final random ones, and the first layers don&#39;t need too much learning. 4x10-6 is the suggested learning rate. . Let&#39;s train the whole model with the new learning rate: . learn.fit_one_cycle(3, lr_max=4.4e-6) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.053950 | 0.435164 | 0.187500 | 0.812500 | 03:24 | . 1 | 0.053677 | 0.220901 | 0.062500 | 0.937500 | 03:23 | . 2 | 0.047155 | 0.361383 | 0.125000 | 0.875000 | 03:23 | . 87.5% accuracy. Not bad for a start, but we will try ways to improve it. . Let us see how our model is doing by inspecting the Confusion Matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Four &#39;NORMAL&#39; images are being classified as &#39;PNEUMONIA&#39;. Can this be caused because our model doesn&#39;t have enough examples of the &#39;NORMAL&#39; class to learn about? Let us investigate. . Solving the Imbalance Problem . The solution to the problem, as with many solutions to problems in Deep Learning, is simple and something that can be implemented easily. Quoting from their conclusions in the paper: . The method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling. | Oversampling should be applied to the level that completely eliminates the imbalance. | Oversampling does not cause over tting of convolutional neural networks. | . Basically, Oversampling is artificially making the minority class bigger by replicating it a couple of times. The paper recommends we replicate it until is completely eliminates the imbalance, therefore, our new oversampled &#39;NORMAL&#39; class is going to be the original images, repeated three times. And we don&#39;t have to worry about overfitting of our model too! . os_normal = get_image_files(path/&#39;train&#39;/&#39;NORMAL&#39;) * 3 pneumonia = get_image_files(path/&#39;train&#39;/&#39;PNEUMONIA&#39;) print(f&quot;Normal Images: {len(os_normal)}. Pneumonia Images: {len(pneumonia)}&quot;) . Normal Images: 4023. Pneumonia Images: 3875 . data = [[&#39;Normal&#39;, len(os_normal)], [&#39;Pneumonia&#39;, len(pneumonia)]] os_df = pd.DataFrame(data, columns=[&#39;Class&#39;, &#39;Count&#39;]) sns.barplot(x=os_df[&#39;Class&#39;], y=os_df[&#39;Count&#39;]); . After the Oversampling, the distribution between the classes is almost at per. Now our dataset it balanced and we can train a new model on this Balanced Data. . Now we need a new way to split our dataset when loading it to a DataLoader. Our new Oversampled Path is going to be the Oversampled &#39;NOMARL&#39; class, the original &#39;PNEUMONIA&#39; and the validation data. . Then we create two variables, train_idx and val_idx, that represent the indexes of the respective category of the images, whether train or validation. . os_path = os_normal + pneumonia + val train_idx = [i for i, fname in enumerate(os_path) if &#39;train&#39; in str(fname)] val_idx = [i for i, fname in enumerate(os_path) if &#39;val&#39; in str(fname)] . L(train_idx), L(val_idx) . ((#7898) [0,1,2,3,4,5,6,7,8,9...], (#16) [7898,7899,7900,7901,7902,7903,7904,7905,7906,7907...]) . Now we have 7898 images in the Train instead of the original 5216, and we still have 16 Validation images. We load them up in a dataloaders object, which our learner expects, find the new optimal learning rate and train the model: . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=lambda x: [train_idx, val_idx], item_tfms=Resize(460), batch_tfms=augs ) . dls = dblock.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy]) learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.0030199517495930195) . learn.fit_one_cycle(3, lr_max=2.5e-2) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.346110 | 0.308598 | 0.125000 | 0.875000 | 02:18 | . 1 | 0.196921 | 0.040522 | 0.000000 | 1.000000 | 02:17 | . 2 | 0.107927 | 0.038475 | 0.000000 | 1.000000 | 02:19 | . After just three epochs, we get 100% accuracy on the Validation Set. The Oversampling Solution worked well for us. . However, as I mentioned before, we only have 16 images on the Validation Set, so its not a good measure on how well our model generalizes. . So I combined the Validation and Test Set into one, and used that as my Validation Set to test how well my model generalizes. . merged_path = os_normal + pneumonia + val + test train_idx = [i for i, fname in enumerate(merged_path) if &#39;train&#39; in str(fname)] val_idx = [i for i, fname in enumerate(merged_path) if &#39;train&#39; not in str(fname)] . L(train_idx), L(val_idx) . ((#7898) [0,1,2,3,4,5,6,7,8,9...], (#640) [7898,7899,7900,7901,7902,7903,7904,7905,7906,7907...]) . We now have 640 images as our validation. How does our model perform with this new data? . learn.fit_one_cycle(5, lr_max=1e-4) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.087615 | 0.043269 | 0.014062 | 0.985937 | 02:30 | . 1 | 0.079633 | 0.041382 | 0.015625 | 0.984375 | 02:30 | . 2 | 0.067601 | 0.054884 | 0.018750 | 0.981250 | 02:31 | . 3 | 0.053627 | 0.027576 | 0.006250 | 0.993750 | 02:30 | . 4 | 0.037838 | 0.025329 | 0.004688 | 0.995313 | 02:27 | . 99.5% accuracy after 5 epochs looks good, looks like our model generalizes well. . See you next time! .",
            "url": "https://jimmiemunyi.github.io/blog/begginer/2020/10/14/Pneumonia-Classification-with-Deep-Learning.html",
            "relUrl": "/begginer/2020/10/14/Pneumonia-Classification-with-Deep-Learning.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Training State of the Art Models with Little Data and Compute with Transfer Learning.",
            "content": "Introduction . Today, we are going to have a beginner&#39;s look into an interesting approach in Deep Learning, called Transfer Learning. We are also going to build a simple Computer Vision model to demonstrate a working example of Transfer Learning. Our model will be a classifier that differentiates between Millipedes, Centipedes and Spiders. . We are going to be using fastai and PyTorch for this tutorial. More specifically, we are going to be using Google&#39;s free service Colab that gives us free GPU to do Deep Learning. . What is Transfer Learning? . So what really is it Transfer Learning? . In simple terms, Transfer Learning is the approach of transferring knowledge from one Deep Learning Model to another. In more technical terms, Transfer Learning is the approach of using an already pretained model, and adapting it to a new problem. . This simple approach helps developers get state of the art results with little data and little compute. . Training a model from scratch requires a lot of compute and a lot of data. For example the pretrained model we are going to use was trained on ImageNet database which contains 1.2 million images with 1000 categories. In practice, very few people train the enitre Network from scratch, we often leverage the knowledge gained from these pretrained models and adapt them to our specific dataset. . To learn more about Transfer Learning, you can use these notes. . A Little Intuition Before We Begin. . Deep Learning models consists of many layers inside them, each learning its own unique features. In 2013, two researches published a paper called Visualizing and Understanding Convolutional Networks that helped visualize what is going on inside the layers and what they actually learn. In the interest of keeping this post beginner friendly, I won&#39;t go much into the technical details of the paper but, here are some images showing what the layers in the neural network learn. . In the first layer, the two reasearchers showed that the network learns general features like diagonal, vertical and horizontal edges. These are the building blocks. . . In the second layer, the Network starts to learn simple patterns that are also general to any Computer Vision Data like circles, etc. . And it keeps on improving layer by layer, building from the building blocks. . . So as you can see, the first layers of a Convolutional Network learn general patterns that are common to all images. This is why we don&#39;t want to discard these knowledge because it can be used for any dataset. . What actually happens in transfer learning, specifically for Computer Vision Tasks is the first layers are freezed (no learning goes on) and the final layer (the one that actually does the classification e.g dog from cat) is chopped of and replaced with a classification layer that is specific to the dataset, i.e our final layer will be trained to specifically distinguish Millipedes, Centipedes and Spiders. . Let&#39;s get straight into the practical example, shall we? . Training A Computer Vision Model Using Transfer Learning . Let&#39;s start by handling our imports: . from fastai.vision.all import * . For the DataSet, we are going to scrap the internet for centipedes, millipedes and spiders. We are going to use a very handy tool, jmd_imagescraper, that uses DuckDuckGo for the image scraping and returns some nice images. The developer also provides a nice ImageCleaner that we are going to use later to clean up the dataset. Let&#39;s import them too. . from jmd_imagescraper.core import * from jmd_imagescraper.imagecleaner import * . We create a directory called &#39;data&#39;. We then use the image scrapper to get the images of the three classes we are interested in and save them each to their specific directories inside the &#39;data&#39; directory, i.e. the centipedes will be stored inside a directory called &#39;Centipedes&#39; and the millipedes will be stored inside the &#39;Millipede&#39; directory and likewise for the spiders (This arrangement is going to prove useful later!). We download 150 images for each. . root = Path().cwd()/&quot;data&quot; duckduckgo_search(root, &#39;Centipede&#39;, &#39;centipede&#39;, max_results=150) duckduckgo_search(root, &#39;Millipede&#39;, &#39;millipede&#39;, max_results=150) duckduckgo_search(root, &#39;Spider&#39;, &#39;spider&#39;, max_results=150) . Let us see how our &#39;data&#39; directory looks after the downloading completes: . path = Path(&#39;data&#39;) . path.ls() . (#3) [Path(&#39;Spider&#39;),Path(&#39;Centipede&#39;),Path(&#39;Millipede&#39;)] . As you can see, we have three directories inside, each corresponding to the images it containes. If we look inside a specific directory, e.g. Centipede, we see the individual images downloaded, and the total number of images downloaded (150) prefixed before the list: . (path/&#39;Centipede&#39;).ls() . (#150) [Path(&#39;Centipede/135_dd009ed0.jpg&#39;),Path(&#39;Centipede/084_69e0099b.jpg&#39;),Path(&#39;Centipede/129_5409e84e.jpg&#39;),Path(&#39;Centipede/077_cc3b3dd9.jpg&#39;),Path(&#39;Centipede/097_cdfd1abf.jpg&#39;),Path(&#39;Centipede/030_55b8c176.jpg&#39;),Path(&#39;Centipede/090_ef7667e5.jpg&#39;),Path(&#39;Centipede/028_5b5b8f46.jpg&#39;),Path(&#39;Centipede/052_ec993151.jpg&#39;),Path(&#39;Centipede/056_86c51270.jpg&#39;)...] . Okay, now that we have got our images ready, we can begin the next step which is processing them. We use a handy function provided by fastai called get_image_files, which simply recursively goes through the directory and gets all the images inside them. . fns = get_image_files(path) fns . (#450) [Path(&#39;Spider/019_ffed6440.jpg&#39;),Path(&#39;Spider/100_59bd4277.jpg&#39;),Path(&#39;Spider/056_21ce5818.jpg&#39;),Path(&#39;Spider/114_33c06a31.jpg&#39;),Path(&#39;Spider/001_f7a867bc.jpg&#39;),Path(&#39;Spider/139_3d7b9ec9.jpg&#39;),Path(&#39;Spider/007_f8419240.jpg&#39;),Path(&#39;Spider/113_3082658a.jpg&#39;),Path(&#39;Spider/135_347f4e6e.jpg&#39;),Path(&#39;Spider/144_e94c648a.jpg&#39;)...] . We have 450 images, which makes sense. Did any image get corrupted during downloading? Let us verify the images. . failed = verify_images(fns) failed . (#0) [] . Luckily, no image got corrupted. Good, now let&#39;s go on. . Let us open one and see how it looks: . im = Image.open(fns[2]) im.to_thumb(128, 128) . So far everything is looking good! . Since we have gotten the images, now we can start processing them in a format that our learner expects. We are going to use the DataBlock API from fastai. . I am going to give a brief explanation of what is going on, but I highly recommend going through their documentaion about the DataBlock API, where they explain everything in detail. . Let us first see how the code looks like: . images = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(valid_pct=0.2, seed=42), item_tfms=RandomResizedCrop(224, min_scale=0.3), batch_tfms=aug_transforms() ) . Let us go step by step: . blocks=(ImageBlock, CategoryBlock) . This simply tells the dataloader the format of the data it is receiving, i.e. here, our independent variable is going to be and Image, hence the ImageBlock, and the labels or the dependent variables are going to be a category (either &#39;Centipede&#39;, &#39;Millipede&#39;, or &#39;Spider&#39;) . get_items=get_image_files . This tells our dataloader how to get the items, using the get_image_files we used before . get_y=parent_label . This instructs our dataloader on how to get the labels of the images, by getting the parent name of the directory the image is in (That&#39;s why we arranged the pictures in images in their repsective directories). . splitter=RandomSplitter(valid_pct=0.2, seed=42) . This provides a way of splitting the dataset into a training and a validation set. Here we split the validation set into 20% of the total data. The seed option is there to ensure we get the same validation set every time. . item_tfms=RandomResizedCrop(224, min_scale=0.3) . This is simply a transformation done on every image individually. Here we resize the images to 224 x 224. Images should be the same size when fed into the Neural Network. We go an extra step of randomly picking a different crop of the images every time, i.e. a minimum of 30% of the total image every time. Randomly picking a different section of the image every time helps the Network generalize well to new data. . And finally this, . batch_tfms=aug_transforms() . performs data augmentation on the images. Data Augmentation deserves a whole post by itself to explain, but for intuition on why we do this, let me give a brief explanation. When using our model in the real world, people will provide images in very different formats, taken from different angles, some from cameras with low pixel capturing capabilities which provides somewhat blurry images. But we still need the model to generalize well to all of these cases! Hence data augmentation. Data Augmentation transforms the images to different versions, flipping it, rotating it, darkening it and many other transforms, to help the model generalize well in the real world. We use a batch transform here that applies the transforms in batches in the GPU which is way faster. . Let us load the images into a dataloader, which is what the learner expects, and show one batch of the images. . dls = images.dataloaders(path) . dls.show_batch(max_n=9) . As you can see, the images are being Randomly Resized, cropping every time to 30% of the image. . Let us see what the augmentation transforms did to our data, by adding the unique parameter: . dls.train.show_batch(max_n=8, nrows=2, unique=True) . As you can see, these all are the same images but transformed differently. . Now we are ready to create the model. . Training The Model . Remember all the talk of using a pretrained model? Well, here is where we apply it. . We are using the resnet18 pretrained model from PyTorch and fine tuning it for 5 epochs for our specific dataset. The &#39;18&#39; suffix simply means it has 18 layers, which is going to be sufficient for our simple model. However, there are deeper models like resnet34, resnet50, resnet101 and resnet152 with the respective number of layers. Deeper models take more time to train, and often produce better results but not always! As a rule of thumb, start simple then upgrade if need be. . We load our dataloaders (dls) created earlier and we are going to output &#39;error_rate&#39; and &#39;accuracy&#39; as our metrics, to guide us on how well our model is performing. . We are going to use a cnn_learner which is simply a Convolutional Neural Network Learner which is the type of Neural Network widely used for Computer Visions tasks. . learn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy]) learn.fine_tune(5) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate accuracy time . 0 | 1.545470 | 0.389558 | 0.155556 | 0.844444 | 00:03 | . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.454676 | 0.267906 | 0.100000 | 0.900000 | 00:03 | . 1 | 0.342936 | 0.232648 | 0.066667 | 0.933333 | 00:03 | . 2 | 0.291200 | 0.193626 | 0.077778 | 0.922222 | 00:03 | . 3 | 0.237957 | 0.190752 | 0.066667 | 0.933333 | 00:03 | . 4 | 0.205695 | 0.206321 | 0.066667 | 0.933333 | 00:03 | . After 5 epochs, we get an error rate of 6.7% which corresponds to an accuracy of 93.3%. That is really good considering our small dataset and the time we used to train this model, approximately 20 seconds, but as you will see, we can improve this. . You may be asking yourself why we didn&#39;t clean the dataset first before training. It is good to train your model as soon as possible to provide you with a baseline which you can start improving from. And we will clean the dataset later, with the help of the training results and then retrain with a clean dataaset and see if it improves. . Let us inspect what errors our initial model is making. The Classification Confusion Matrix can aid in displaying this in a good format. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . The dark colors on that diagonal indicate that the model is perfoming well. But it is still making mistakes, for example it classified Centipedes as Millipedes 4 times and Spiders as Centipedes twice. . Let us see the specific images it is getting wrong and try to understand why it is confusing them by plotting the top losses of our model. . interp.plot_top_losses(5, nrows=5) . Straight away we can see that some of the mistakes it is making is because of unclean data. For example the 2nd and 4th images have nothing to do with our data. . This is why we need to clean the data. As you can see, training the model first helps us with the cleaning process. . We are going to use the ImageCleaner provides by the jmd_imagescrapper developer. . display_image_cleaner(root) . . I did deleted a few of the images from the datasets that didn&#39;t fit the criteria and we were left with 394 images (but useful ones!). . fns = get_image_files(path) fns . (#394) [Path(&#39;Spider/019_ffed6440.jpg&#39;),Path(&#39;Spider/100_59bd4277.jpg&#39;),Path(&#39;Spider/056_21ce5818.jpg&#39;),Path(&#39;Spider/114_33c06a31.jpg&#39;),Path(&#39;Spider/001_f7a867bc.jpg&#39;),Path(&#39;Spider/139_3d7b9ec9.jpg&#39;),Path(&#39;Spider/007_f8419240.jpg&#39;),Path(&#39;Spider/113_3082658a.jpg&#39;),Path(&#39;Spider/135_347f4e6e.jpg&#39;),Path(&#39;Spider/144_e94c648a.jpg&#39;)...] . Okay, now we create a new dataloader with the clean images. . dls = images.dataloaders(path) . Will training with only clean data help improve our model? Lets train a new model and see. We are going to use the exact details we used before, but I am fine-tuning for 10 epochs this time. . learn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy]) learn.fine_tune(10) . epoch train_loss valid_loss error_rate accuracy time . 0 | 2.070883 | 0.254761 | 0.076923 | 0.923077 | 00:02 | . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.478606 | 0.169040 | 0.038462 | 0.961538 | 00:02 | . 1 | 0.413722 | 0.118356 | 0.038462 | 0.961538 | 00:02 | . 2 | 0.333819 | 0.103353 | 0.038462 | 0.961538 | 00:02 | . 3 | 0.260725 | 0.119273 | 0.025641 | 0.974359 | 00:02 | . 4 | 0.213143 | 0.118922 | 0.025641 | 0.974359 | 00:02 | . 5 | 0.185268 | 0.092165 | 0.025641 | 0.974359 | 00:02 | . 6 | 0.156762 | 0.087852 | 0.012821 | 0.987179 | 00:02 | . 7 | 0.138017 | 0.083028 | 0.025641 | 0.974359 | 00:02 | . 8 | 0.118409 | 0.083742 | 0.025641 | 0.974359 | 00:02 | . 9 | 0.111713 | 0.082776 | 0.025641 | 0.974359 | 00:02 | . We went upto an error rate of just 2.6% which means that our model is correct 97.4% of the time! . As you have seen practically, Transfer Learning is a very important technique in Deep Learning that can go a long way. We only used 394 images here and trained for approximately for 20 seconds and got a model which is pretty accurate. . Stay tuned for more. .",
            "url": "https://jimmiemunyi.github.io/blog/begginer/2020/10/02/Transfer-Learning.html",
            "relUrl": "/begginer/2020/10/02/Transfer-Learning.html",
            "date": " • Oct 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Jimmie Munyi. . I am a developer, currently self teaching Machine Learning and Deep Learning. . This is my technical blog where I post stuff I’m working on and stuff I have learnt. . I am a student, graduation set for 2021. .",
          "url": "https://jimmiemunyi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jimmiemunyi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}